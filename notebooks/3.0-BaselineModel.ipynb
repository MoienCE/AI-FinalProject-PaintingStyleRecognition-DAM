{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2165057",
   "metadata": {},
   "source": [
    "# Step 4: Baseline Model Design\n",
    "\n",
    "In this notebook, we define and instantiate the **Baseline Model**. \n",
    "This model serves as a benchmark for the project. In Phase 2, we will compare this simple architecture against advanced Transfer Learning models (like ResNet50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5ad318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from models import SimpleCNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eb866",
   "metadata": {},
   "source": [
    "### 1. Model Architecture Definition\n",
    "We use a standard Convolutional Neural Network (CNN) with 3 convolutional blocks followed by fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e750c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "# Note: WikiArt typically has 27 styles, but we can detect this dynamically later.\n",
    "model = SimpleCNN(num_classes=27).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f8549",
   "metadata": {},
   "source": [
    "### 2. Scientific Justification (For Report)\n",
    "\n",
    "**Why this architecture?**\n",
    "1.  **Simplicity & Speed:** The `SimpleCNN` has significantly fewer parameters than modern deep networks. This allows for rapid iteration and debugging of the training pipeline in Phase 1.\n",
    "2.  **Benchmarking:** By starting with a shallow network (3 layers), we establish a **lower-bound performance**. This scientifically proves the necessity of deeper networks and Transfer Learning in Phase 2. If this simple model achieves 30% accuracy, and ResNet achieves 80%, we can quantify the value of \"Depth\" and \"Pre-training\".\n",
    "3.  **Resource Constraints:** Training this model from scratch is feasible on mid-range GPUs (like RTX 4060) without requiring massive pre-training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc131be",
   "metadata": {},
   "source": [
    "### 3. Smoke Test (Forward Pass)\n",
    "We pass a random tensor through the model to ensure dimensions are calculated correctly and there are no runtime errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ad49e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Forward pass successful.\n",
      "Input Shape: torch.Size([4, 3, 224, 224])\n",
      "Output Shape: torch.Size([4, 27]) (Should be [4, 27])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input batch: (Batch=4, Channels=3, Height=224, Width=224)\n",
    "dummy_input = torch.randn(4, 3, 224, 224).to(device)\n",
    "\n",
    "try:\n",
    "    output = model(dummy_input)\n",
    "    print(\"✅ Forward pass successful.\")\n",
    "    print(f\"Input Shape: {dummy_input.shape}\")\n",
    "    print(f\"Output Shape: {output.shape} (Should be [4, 27])\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in forward pass: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff6102",
   "metadata": {},
   "source": [
    "### 4. Parameter Count\n",
    "Calculating the complexity of our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ff3361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 51,488,283\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
